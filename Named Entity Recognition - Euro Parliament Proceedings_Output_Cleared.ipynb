{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a16225",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Euro Parliamentary Proceedings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a259a4",
   "metadata": {},
   "source": [
    "This project involves using Named Entity Recognition to identify prominent entities (dates, persons, organisation, etc.) mentioned within European Parliamentary proceedings corpus. Identifying entities would allow institutions working with European data to access correctly tagged information more quickly than if they were required to browse the full archives for relevant information. The raw dataset can be found on Kaggle at: https://www.kaggle.com/datasets/nltkdata/europarl\n",
    "\n",
    "This project only features the English documents listed under 'en' in this dataset. However, it may be possible to use other language models to parse information from other translations in the main corpus. The goal is to produce a dataframe that lists individual documents alongside their respective entities.\n",
    "\n",
    "The project has the following main sections:\n",
    "- Importing and Preprocessing text files\n",
    "- Rendering HTML documents labelling entities\n",
    "- Creating a dataframe listing the entities within each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f2dca2",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing text files. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365c26f",
   "metadata": {},
   "source": [
    "First, let's import the library using glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import glob\n",
    "\n",
    "# The document files are contained in this folder\n",
    "folder = \"C:/Users/dalin/Dropbox/MachineLearning/Entity Recognition/Euro_Parliament/txt/en/\"\n",
    "\n",
    "# List all the .txt files and sort them alphabetically\n",
    "files = glob.glob(folder + \"*.txt\")\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ffc88",
   "metadata": {},
   "source": [
    "NER using Spacy:\n",
    "\n",
    "Spacy is an open-source Natural Language Processing library that can be used for various tasks. It has built-in methods for Named Entity Recognition. Spacy has a fast statistical entity recognition system.\n",
    "\n",
    " We can use spacy very easily for NER tasks. Though often we need to train our own data for business-specific needs, the spacy model general performs well for all types of text data.  \n",
    "\n",
    "Let us get started with the code, first we import spacy and proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the spacy model\n",
    "NER = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2db03",
   "metadata": {},
   "source": [
    "Now, we'll preprocess the text to remove unnecessary characters for a cleaner text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1db710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the lists that will contain the texts and titles of each document\n",
    "txts = []\n",
    "titles = []\n",
    "\n",
    "for n in tqdm(files):\n",
    "    # Open each file\n",
    "    f = open(n, encoding='utf-8-sig')\n",
    "    # Remove all non-alpha-numeric characters except periods, question marks, exclamation marks. f.read() reads the text and ' ' replaces non-alphanumeric characters with a space.\n",
    "    data = re.sub('[^a-zA-Z0-9_.?!]+', ' ', f.read())\n",
    "    # Store the texts and titles of the books in two separate lists\n",
    "    txts.append(data)\n",
    "    titles.append(os.path.basename(n).replace(\".txt\", \"\"))\n",
    "\n",
    "#Â Print the length, in characters, of each text\n",
    "[len(t) for t in txts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04986bc0",
   "metadata": {},
   "source": [
    "The titles of each document will be converted to a dataframe column. This column can be used as an index to identify each document and its respective entities in the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e062442",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dataframe = pd.DataFrame(titles)\n",
    "title_dataframe.rename(columns = {0 : 'File_Name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985e7e7",
   "metadata": {},
   "source": [
    "This dataframe will be used to indicate the filename in the final table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a777743",
   "metadata": {},
   "source": [
    "## Rendering HTML documents labelling entities ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a735cb",
   "metadata": {},
   "source": [
    "In the next section, we'll render html documents clearly highlighting entities within the documents themselves. This should allow a reader to quickly scan a given document in the corpus and see what entities have been tagged. (It should be noted that some documents may be short enough that no entities are discovered.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7351ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for i, t in tqdm(zip(titles, txts)): \n",
    "    text= NER(t)\n",
    "    # Creates an html render for each document highlighting the location of known entities.\n",
    "    html = displacy.render(text, style=\"ent\", jupyter=False)\n",
    "    # The filename for each document will be created using the titles list created earlier.\n",
    "    file_name = i + \".html\"\n",
    "    with open(\"C:/Users/dalin/Dropbox/MachineLearning/Entity Recognition/Euro_Parliament/txt/en/Renders/\" + file_name, 'w+', encoding=\"utf-8\") as fp:\n",
    "        fp.write(html)\n",
    "        fp.close()\n",
    "#    output_path = Path(\"C:/Users/dalin/Dropbox/MachineLearning/Entity Recognition/Euro_Parliament/txt/en/Preprocessing_Sample/Renders/\" + file_name)\n",
    "#    output_path.open(\"w\", encoding=\"utf-8\").write(svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f0e251",
   "metadata": {},
   "source": [
    "## Creating a dataframe listing the entities within each document ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7f89b",
   "metadata": {},
   "source": [
    "Now we will generate a dataframe that adds each document's entities as a new row. The final dataframe can be used as a database to identify the location of prominent entites within the Euro Parl corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7889392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for t in tqdm(txts):\n",
    "    text= NER(t)\n",
    "    entity_list = []\n",
    "    for word in text.ents:\n",
    "        word_label = (word.text, word.label_)\n",
    "        entity_list.append(word_label)\n",
    "    df = df.append(pd.DataFrame([entity_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60599051",
   "metadata": {},
   "source": [
    "This process will generate an extra index column that we do not need, so this can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the extra index column\n",
    "del df[df.columns[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dda330",
   "metadata": {},
   "source": [
    "Join title dataframe and entity dataframe together so that the title of each document fills the first column under \"File_Name\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2621413",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = title_dataframe.join(df)\n",
    "main_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb2900",
   "metadata": {},
   "source": [
    "This dataframe can be further summarized info a final dataframe that lists how many times each entity appears in each document. This method will generate a sparse matrix that can be used to locate the importance of an entity within each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = df.apply(pd.value_counts, axis=1)\n",
    "summary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6070f",
   "metadata": {},
   "source": [
    "Add the document titles back to this dataframe as the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = title_dataframe.join(summary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f2098",
   "metadata": {},
   "source": [
    "Save Table as a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14132f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"C:/Users/dalin/Dropbox/MachineLearning/Entity Recognition/Euro_Parliament/txt/en/Entity_list.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
